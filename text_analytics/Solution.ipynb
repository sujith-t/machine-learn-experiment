{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7aa0251-d8b5-4f74-8625-cf7d360fc828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "# Importing all libraries upfront\n",
    "\n",
    "# General utilities\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "from datetime import datetime, date\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "\n",
    "# Twitter data fetching\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "# NLTK specific libraries\n",
    "import nltk\n",
    "from nltk.corpus import treebank, wordnet as wn\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f91b73-a31b-4dfb-9aed-ff25f9b3c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing necessary data\n",
    "\n",
    "#initialize handler dataframe\n",
    "handlers = {\"handlers\":[\"Dailymirror_SL\", \"colombotelegrap\", \"NewsfirstSL\", \"theisland_lk\", \"NewsWireLK\", \n",
    "            \"adaderana\", \"DailyNews_lk\", \"TimesOnlineLK\", \"FT_SriLanka\", \"CeylonToday\"]}\n",
    "\n",
    "handlers_df = pd.DataFrame(handlers)\n",
    "handlers_df[\"followers\"] = 0\n",
    "handlers_df[\"friends\"] = 0\n",
    "handlers_df[\"tweets\"] = 0\n",
    "\n",
    "# tweets retrieval periods\n",
    "to_date = datetime(2023, 6, 29)\n",
    "from_date = datetime(to_date.year - 1, to_date.month, to_date.day)\n",
    "\n",
    "# data save and retrieval directory\n",
    "data_dir = r'C:\\Users\\SujithThillymplam\\Documents\\machine-learn-experiment\\text_analytics'\n",
    "\n",
    "# additional REST endpoint headers\n",
    "api_headers = {\"Authorization\": \"Bearer AAAAAAAAAAAAAAAAAAAAANRILgAAAAAAnNwIzUejRCOuH5E6I8xnZz4puTs%3D1Zv7ttfk8LF81IUq16cHjhLTvJu4FA33AGWWjCpTnA\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b871a8eb-3458-47d2-9351-3d9564059a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to generate summary counts for followers, friends and number of tweets for each handler \n",
    "\n",
    "# function to count followers and friends\n",
    "def followers_friends_count(df, header_map={}, option='friends'):\n",
    "    for inx, row in df.iterrows():\n",
    "        url = \"https://api.twitter.com/1.1/\" + option + \"/ids.json?screen_name=\" + row[\"handlers\"] + \"&count=5000\"\n",
    "        fjson = requests.get(url, headers=api_headers).json()\n",
    "        fcount = len(fjson[\"ids\"])\n",
    "        next_cursor = fjson[\"next_cursor\"] \n",
    "    \n",
    "        while next_cursor > 0:\n",
    "            sub_url = url + \"&cursor=\" + str(fjson[\"next_cursor\"])\n",
    "            fjson = requests.get(sub_url, headers=header_map).json()\n",
    "            next_cursor = fjson[\"next_cursor\"]\n",
    "            fcount += len(fjson[\"ids\"])\n",
    "        \n",
    "        df.at[inx, option] = fcount\n",
    "        print(\"%s : %s - %s\" % (option, row[\"handlers\"], fcount))\n",
    "        \n",
    "# function to count tweets for a given period of time for all news handlers        \n",
    "def tweet_counts_for_periods(df, from_date, to_date):\n",
    "    \n",
    "    for inx, row in df.iterrows():\n",
    "        tweet_count = 0;\n",
    "        query = '@' + row[\"handlers\"]\n",
    "        tquery = \"%s lang:en until:%s since:%s\" % (query, to_date.strftime(\"%Y-%m-%d\"), from_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        itr = sntwitter.TwitterSearchScraper(tquery).get_items()\n",
    "        for tweet in itr:\n",
    "            tweet_count += 1\n",
    "        \n",
    "        df.at[inx, \"tweets\"] = tweet_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58c6d1d-a1e4-4f01-b5ee-b78113337e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the above function and generate the summary\n",
    "# note - all 3 function execution is time consuming, executed individually and saved in a CSV file for later reference\n",
    "\n",
    "followers_friends_count(handlers_df, api_headers)\n",
    "followers_friends_count(handlers_df, api_headers, 'followers')\n",
    "tweet_counts_for_periods(handlers_df, from_date, to_date)\n",
    "\n",
    "# handlers_df.to_csv (data_dir + \"\\handlers.csv\", index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74c662bf-3d10-41a4-b699-20c0a9d53386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handlers</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>588872</td>\n",
       "      <td>35</td>\n",
       "      <td>34677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>colombotelegrap</td>\n",
       "      <td>42406</td>\n",
       "      <td>2</td>\n",
       "      <td>1198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsfirstSL</td>\n",
       "      <td>525162</td>\n",
       "      <td>8</td>\n",
       "      <td>20932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theisland_lk</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "      <td>4743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsWireLK</td>\n",
       "      <td>222255</td>\n",
       "      <td>2</td>\n",
       "      <td>39698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adaderana</td>\n",
       "      <td>533456</td>\n",
       "      <td>14</td>\n",
       "      <td>9114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DailyNews_lk</td>\n",
       "      <td>14858</td>\n",
       "      <td>93</td>\n",
       "      <td>4314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TimesOnlineLK</td>\n",
       "      <td>17092</td>\n",
       "      <td>7</td>\n",
       "      <td>2736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FT_SriLanka</td>\n",
       "      <td>80772</td>\n",
       "      <td>8</td>\n",
       "      <td>2857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CeylonToday</td>\n",
       "      <td>62797</td>\n",
       "      <td>33</td>\n",
       "      <td>3972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          handlers  followers  friends  tweets\n",
       "0   Dailymirror_SL     588872       35   34677\n",
       "1  colombotelegrap      42406        2    1198\n",
       "2      NewsfirstSL     525162        8   20932\n",
       "3     theisland_lk        261        0    4743\n",
       "4       NewsWireLK     222255        2   39698\n",
       "5        adaderana     533456       14    9114\n",
       "6     DailyNews_lk      14858       93    4314\n",
       "7    TimesOnlineLK      17092        7    2736\n",
       "8      FT_SriLanka      80772        8    2857\n",
       "9      CeylonToday      62797       33    3972"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question (a) - 10 popular news sources\n",
    "# news handlers|no of followers|no of followings|no of tweets for last 1 year\n",
    "# Read from handlers.csv file and display the stats\n",
    "# The values obtained here are generated by executing the above given functions (time consuming)\n",
    "\n",
    "handlers_df = pd.read_csv(data_dir + \"\\handlers.csv\")\n",
    "handlers_df[[\"handlers\", \"followers\", \"friends\", \"tweets\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d92dd8d-080b-4ffe-a0c7-a5b00850045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the newslinks for every every handler\n",
    "\n",
    "def find_tweets_with_news_links(handler_list, from_date, to_date, size=0):\n",
    "    filtered_tweets = []\n",
    "    \n",
    "    for handler_name in handler_list:\n",
    "        tquery = \"%s lang:en until:%s since:%s\" % (('@' + handler_name), to_date.strftime(\"%Y-%m-%d\"), from_date.strftime(\"%Y-%m-%d\"))\n",
    "        tweets = sntwitter.TwitterSearchScraper(tquery).get_items()\n",
    "        web_url_regex = \"(https?://t\\.co/([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)\"\n",
    "\n",
    "        for t in tweets:\n",
    "            urls = re.findall(web_url_regex, t.rawContent)\n",
    "            news_url = ''\n",
    "            for x in urls:\n",
    "                news_url = x[0].strip()\n",
    "                filtered_tweets.append([t.id, handler_name, t.date, news_url])\n",
    "            \n",
    "            if(size > 0 and len(filtered_tweets) > size):\n",
    "                break\n",
    "    \n",
    "        if(size > 0 and len(filtered_tweets) > size):\n",
    "            break\n",
    "            \n",
    "    headers = ['id', 'source', 'date', 'url']\n",
    "    return pd.DataFrame(filtered_tweets, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6a6d0e-5cf2-44e9-97a1-8ebfbfe407ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the news extractor and generate newslinks\n",
    "# save to a CSV file for future reference so that the contents can be retrieved later\n",
    "\n",
    "news_df = find_tweets_with_news_links(handlers[\"handlers\"], from_date, to_date)\n",
    "#news_df.to_csv (data_dir + \"\\\\news_contents.csv\", index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd4ef725-60da-490e-b47c-cdb807ed40a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 11:19:41+00:00</td>\n",
       "      <td>https://t.co/f9WwIKUC6k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 11:19:01+00:00</td>\n",
       "      <td>https://t.co/osPbuVCcd5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 11:07:33+00:00</td>\n",
       "      <td>https://t.co/aANaVO4QcW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 08:04:39+00:00</td>\n",
       "      <td>https://t.co/8c8Wu4kGPy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 08:07:02+00:00</td>\n",
       "      <td>https://t.co/IPFcbc0pVH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:44:42+00:00</td>\n",
       "      <td>https://t.co/9WFsFwkQWU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:44:01+00:00</td>\n",
       "      <td>https://t.co/vEsOHG1z5w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:43:13+00:00</td>\n",
       "      <td>https://t.co/2ilBkJNFa7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:42:29+00:00</td>\n",
       "      <td>https://t.co/GDXR3AFJdW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:41:48+00:00</td>\n",
       "      <td>https://t.co/05eKZ6PwK3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id          source                       date  \\\n",
       "0  1.67E+18  Dailymirror_SL  2023-06-29 11:19:41+00:00   \n",
       "1  1.67E+18  Dailymirror_SL  2023-06-29 11:19:01+00:00   \n",
       "2  1.67E+18  Dailymirror_SL  2023-06-29 11:07:33+00:00   \n",
       "3  1.67E+18  Dailymirror_SL  2023-06-29 08:04:39+00:00   \n",
       "4  1.67E+18  Dailymirror_SL  2023-06-29 08:07:02+00:00   \n",
       "5  1.67E+18  Dailymirror_SL  2023-06-29 05:44:42+00:00   \n",
       "6  1.67E+18  Dailymirror_SL  2023-06-29 05:44:01+00:00   \n",
       "7  1.67E+18  Dailymirror_SL  2023-06-29 05:43:13+00:00   \n",
       "8  1.67E+18  Dailymirror_SL  2023-06-29 05:42:29+00:00   \n",
       "9  1.67E+18  Dailymirror_SL  2023-06-29 05:41:48+00:00   \n",
       "\n",
       "                       url  \n",
       "0  https://t.co/f9WwIKUC6k  \n",
       "1  https://t.co/osPbuVCcd5  \n",
       "2  https://t.co/aANaVO4QcW  \n",
       "3  https://t.co/8c8Wu4kGPy  \n",
       "4  https://t.co/IPFcbc0pVH  \n",
       "5  https://t.co/9WFsFwkQWU  \n",
       "6  https://t.co/vEsOHG1z5w  \n",
       "7  https://t.co/2ilBkJNFa7  \n",
       "8  https://t.co/GDXR3AFJdW  \n",
       "9  https://t.co/05eKZ6PwK3  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display first 10 saved news urls to verify \n",
    "\n",
    "news_df =  pd.read_csv((data_dir + \"/news_contents.csv\"), engine='python', encoding='latin1')\n",
    "news_df[[\"id\", \"source\", \"date\", \"url\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3915a82-b700-4db3-bc3e-8a1aa6b0f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanding the contraction texts, underneath is a helper function to expand them \n",
    "contraction_map = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# expanding the shorthand words\n",
    "def expand_contractions(sentence, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "    return expanded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df21d571-828d-4816-83b5-2146606ed61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question (b) - extract all the articles from the links discovered above\n",
    "\n",
    "# helper function which retrievs texts for a given url\n",
    "# it extracts texts for the givel htmltags and specific attributes in the tags\n",
    "\n",
    "def extract_web_text(url, tags=['p'], filterAttr={}):\n",
    "    text = ''\n",
    "    html = None\n",
    "    soup = None\n",
    "    title = None\n",
    "    body = None\n",
    "    \n",
    "    try:\n",
    "        html = requests.get(url, timeout=60).content\n",
    "        soup = bs(html, 'html.parser')\n",
    "        title = soup.find('head')\n",
    "        body = soup.find('body')\n",
    "    \n",
    "    except:\n",
    "        print(\"Connectivity error with news url: %s\" % url)\n",
    "        return ''\n",
    "    \n",
    "    if title != None:\n",
    "        for c in title.find_all(tags, filterAttr):\n",
    "            text += c.text.strip()\n",
    "    \n",
    "    if body != None:\n",
    "        for c in body.find_all(tags, filterAttr):\n",
    "            content = re.sub(r'[\\t\\r\\n]', r' ', c.text.strip())\n",
    "            \n",
    "            if len(content) < 19 or (len(content) > 18 and re.search(r\"\\s\", content)):\n",
    "                text += ' ' + content\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "# helper function to remove all unwanted characters from the content\n",
    "# it also expands the contractions by calling expand_contractions() function defined above\n",
    "\n",
    "def clean_text_content(content):\n",
    "    \n",
    "    content = re.sub(r\"[\\'|’]\", \"'\", content.strip())\n",
    "    content = re.sub(r\"[“|”]\", \"'\", content.strip())\n",
    "    content = re.sub(r\"[?|$|&|*|%|@|(|)|~|©|\\\\|™]\", r' ', content)\n",
    "    content = re.sub(r'[\\s]+', r' ', content)\n",
    "    content = expand_contractions(content, contraction_map)\n",
    "    \n",
    "    return content\n",
    "\n",
    "# main function that is called to extract all the links\n",
    "def extract_clean_text(handler_df, links_df, start=0, stop=1000):\n",
    "    handler_map = {}\n",
    "    \n",
    "    for inx, row in handler_df.iterrows():\n",
    "        handler_attr = {}\n",
    "        handler_attr[\"name\"] = row[\"name\"]\n",
    "        handler_attr[\"tags\"] = row[\"content_tags\"].split()\n",
    "        handler_map[row[\"handlers\"]] = handler_attr\n",
    "    \n",
    "    for inx, row in links_df.iterrows():\n",
    "        if inx < start:\n",
    "            continue;\n",
    "            \n",
    "        if inx > stop:\n",
    "            break;\n",
    "        \n",
    "        handler_attr = handler_map[row[\"source\"]]\n",
    "        content = extract_web_text(row[\"url\"], handler_attr[\"tags\"])\n",
    "        content = clean_text_content(content)\n",
    "        print(\"%s url - %s extracted\" % (inx, row[\"url\"]))\n",
    "        \n",
    "        if handler_attr[\"name\"] in content:\n",
    "            links_df.at[inx, \"content\"] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f252790-d8a3-46e2-b67c-7f4ac338757e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 url - https://t.co/f9WwIKUC6k extracted\n",
      "1 url - https://t.co/osPbuVCcd5 extracted\n",
      "2 url - https://t.co/aANaVO4QcW extracted\n",
      "3 url - https://t.co/8c8Wu4kGPy extracted\n",
      "4 url - https://t.co/IPFcbc0pVH extracted\n",
      "5 url - https://t.co/9WFsFwkQWU extracted\n",
      "6 url - https://t.co/vEsOHG1z5w extracted\n",
      "7 url - https://t.co/2ilBkJNFa7 extracted\n",
      "8 url - https://t.co/GDXR3AFJdW extracted\n",
      "9 url - https://t.co/05eKZ6PwK3 extracted\n",
      "10 url - https://t.co/5bkWcAVkW3 extracted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 11:19:41+00:00</td>\n",
       "      <td>https://t.co/f9WwIKUC6k</td>\n",
       "      <td>Gemunu says no to bus fare revision - Breaking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 11:19:01+00:00</td>\n",
       "      <td>https://t.co/osPbuVCcd5</td>\n",
       "      <td>Six standouts set to light up the critical sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 11:07:33+00:00</td>\n",
       "      <td>https://t.co/aANaVO4QcW</td>\n",
       "      <td>Eight foreigners arrested with heroin - Breaki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 08:04:39+00:00</td>\n",
       "      <td>https://t.co/8c8Wu4kGPy</td>\n",
       "      <td>Members appointed to EC and HRC - Breaking New...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 08:07:02+00:00</td>\n",
       "      <td>https://t.co/IPFcbc0pVH</td>\n",
       "      <td>Chief Inspector of police killed in Homagama a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:44:42+00:00</td>\n",
       "      <td>https://t.co/9WFsFwkQWU</td>\n",
       "      <td>300 medical graduates who completed internship...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:44:01+00:00</td>\n",
       "      <td>https://t.co/vEsOHG1z5w</td>\n",
       "      <td>Banking system will not be burdened, EPF funds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:43:13+00:00</td>\n",
       "      <td>https://t.co/2ilBkJNFa7</td>\n",
       "      <td>NMRA lacked professor in pharmacology for mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:42:29+00:00</td>\n",
       "      <td>https://t.co/GDXR3AFJdW</td>\n",
       "      <td>'Presumed human remains' found within wreckage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:41:48+00:00</td>\n",
       "      <td>https://t.co/05eKZ6PwK3</td>\n",
       "      <td>Udeni Rajapaksa to be appointed as new Air For...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id          source                       date  \\\n",
       "0  1.670000e+18  Dailymirror_SL  2023-06-29 11:19:41+00:00   \n",
       "1  1.670000e+18  Dailymirror_SL  2023-06-29 11:19:01+00:00   \n",
       "2  1.670000e+18  Dailymirror_SL  2023-06-29 11:07:33+00:00   \n",
       "3  1.670000e+18  Dailymirror_SL  2023-06-29 08:04:39+00:00   \n",
       "4  1.670000e+18  Dailymirror_SL  2023-06-29 08:07:02+00:00   \n",
       "5  1.670000e+18  Dailymirror_SL  2023-06-29 05:44:42+00:00   \n",
       "6  1.670000e+18  Dailymirror_SL  2023-06-29 05:44:01+00:00   \n",
       "7  1.670000e+18  Dailymirror_SL  2023-06-29 05:43:13+00:00   \n",
       "8  1.670000e+18  Dailymirror_SL  2023-06-29 05:42:29+00:00   \n",
       "9  1.670000e+18  Dailymirror_SL  2023-06-29 05:41:48+00:00   \n",
       "\n",
       "                       url                                            content  \n",
       "0  https://t.co/f9WwIKUC6k  Gemunu says no to bus fare revision - Breaking...  \n",
       "1  https://t.co/osPbuVCcd5  Six standouts set to light up the critical sta...  \n",
       "2  https://t.co/aANaVO4QcW  Eight foreigners arrested with heroin - Breaki...  \n",
       "3  https://t.co/8c8Wu4kGPy  Members appointed to EC and HRC - Breaking New...  \n",
       "4  https://t.co/IPFcbc0pVH  Chief Inspector of police killed in Homagama a...  \n",
       "5  https://t.co/9WFsFwkQWU  300 medical graduates who completed internship...  \n",
       "6  https://t.co/vEsOHG1z5w  Banking system will not be burdened, EPF funds...  \n",
       "7  https://t.co/2ilBkJNFa7  NMRA lacked professor in pharmacology for mont...  \n",
       "8  https://t.co/GDXR3AFJdW  'Presumed human remains' found within wreckage...  \n",
       "9  https://t.co/05eKZ6PwK3  Udeni Rajapaksa to be appointed as new Air For...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question (b) - extract all the articles by calling above functions\n",
    "# following code extracts content from first 10 links and prints them underneath\n",
    "\n",
    "extract_clean_text(handlers_df, news_df, start=0, stop=10)\n",
    "news_df[[\"id\", \"source\", \"date\", \"url\", \"content\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "750a7846-d0f0-4149-b005-ec946cff327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question (b) - function to produce token summary from the above extracted text\n",
    "# This produces no_tokens, no_unique_tokens, no_news_items per newshandler as output\n",
    "\n",
    "def handler_token_summary(handler_df, news_df, start=0, stop=1000):\n",
    "    handler_map = {}\n",
    "    \n",
    "    for inx, row in handler_df.iterrows():\n",
    "        handler_attr = {}\n",
    "        handler_attr[\"no_tokens\"] = 0\n",
    "        handler_attr[\"no_unique_tokens\"] = 0\n",
    "        handler_attr[\"no_news_items\"] = 0\n",
    "        handler_attr[\"unique_tokens\"] = set()\n",
    "        handler_map[row[\"handlers\"]] = handler_attr\n",
    "    \n",
    "    for inx, row in news_df.iterrows():\n",
    "        tokens = nltk.word_tokenize(str(row[\"content\"]))\n",
    "        if row[\"source\"] in handler_map:\n",
    "            handler_attr = handler_map[row[\"source\"]]\n",
    "            handler_attr[\"unique_tokens\"].update(tokens)\n",
    "            handler_attr[\"no_tokens\"] += len(tokens)\n",
    "            handler_attr[\"no_unique_tokens\"] = len(handler_attr[\"unique_tokens\"])\n",
    "            handler_attr[\"no_news_items\"] += 1\n",
    "            handler_map[row[\"source\"]] = handler_attr\n",
    "        \n",
    "        if inx < start:\n",
    "            continue;\n",
    "            \n",
    "        if inx > stop:\n",
    "            handler_attr.pop(\"unique_tokens\")\n",
    "            handler_map[row[\"source\"]] = handler_attr\n",
    "            break;\n",
    "    \n",
    "    return handler_map\n",
    "\n",
    "stats = handler_token_summary(handlers_df, news_df, 0, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db916ea1-a858-4aa4-86e7-f5da32ef8c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handler</th>\n",
       "      <th>no_news_items</th>\n",
       "      <th>no_tokens</th>\n",
       "      <th>no_unique_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>6222</td>\n",
       "      <td>7822119</td>\n",
       "      <td>98964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>colombotelegrap</td>\n",
       "      <td>777</td>\n",
       "      <td>2780678</td>\n",
       "      <td>87700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsfirstSL</td>\n",
       "      <td>3524</td>\n",
       "      <td>11898190</td>\n",
       "      <td>36665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theisland_lk</td>\n",
       "      <td>21</td>\n",
       "      <td>48416</td>\n",
       "      <td>5401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsWireLK</td>\n",
       "      <td>4532</td>\n",
       "      <td>2337881</td>\n",
       "      <td>42052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adaderana</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DailyNews_lk</td>\n",
       "      <td>16</td>\n",
       "      <td>778</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TimesOnlineLK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FT_SriLanka</td>\n",
       "      <td>1794</td>\n",
       "      <td>1557234</td>\n",
       "      <td>42086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CeylonToday</td>\n",
       "      <td>430</td>\n",
       "      <td>387484</td>\n",
       "      <td>14171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           handler  no_news_items  no_tokens  no_unique_tokens\n",
       "0   Dailymirror_SL           6222    7822119             98964\n",
       "1  colombotelegrap            777    2780678             87700\n",
       "2      NewsfirstSL           3524   11898190             36665\n",
       "3     theisland_lk             21      48416              5401\n",
       "4       NewsWireLK           4532    2337881             42052\n",
       "5        adaderana              0          0                 0\n",
       "6     DailyNews_lk             16        778                71\n",
       "7    TimesOnlineLK              0          0                 0\n",
       "8      FT_SriLanka           1794    1557234             42086\n",
       "9      CeylonToday            430     387484             14171"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question (b) Display token summary by handlers\n",
    "# The above handler_token_summary function output is formatted for display\n",
    "\n",
    "summary = []\n",
    "for k, v in stats.items():\n",
    "    summary.append([k, v[\"no_news_items\"], v[\"no_tokens\"], v[\"no_unique_tokens\"]])\n",
    "    \n",
    "summary_df = pd.DataFrame(summary, columns=[\"handler\", \"no_news_items\" , \"no_tokens\", \"no_unique_tokens\"])\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cc0f981-bddf-438f-baee-8ebe65216e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question (b) Data preparation for model building\n",
    "# The under defined helper functions would be combined with the extracted dataset\n",
    "\n",
    "# Lemmatizing the text\n",
    "def lemmatize_text(content, tagger, lemmatizer):\n",
    "    sentences = nltk.sent_tokenize(content)\n",
    "    lemmatized_text = ''\n",
    "    \n",
    "    for s in sentences:\n",
    "        tokens = [t.strip() for t in nltk.word_tokenize(s)]\n",
    "        \n",
    "        tagged_token = tagger.tag(tokens)\n",
    "        lemmatized_tokens = []\n",
    "        \n",
    "        for word, tag in tagged_token:\n",
    "            custom_tag = None\n",
    "            \n",
    "            if tag.startswith('J'):\n",
    "                custom_tag = wn.ADJ\n",
    "            elif tag.startswith('V'):\n",
    "                custom_tag = wn.VERB\n",
    "            elif tag.startswith('N'):\n",
    "                custom_tag = wn.NOUN\n",
    "            elif tag.startswith('R'):\n",
    "                custom_tag = wn.ADV\n",
    "            #else:\n",
    "                #custom_tag = None\n",
    "            \n",
    "            if custom_tag:\n",
    "                lemmatized_tokens.append(lemmatizer.lemmatize(word.lower(), custom_tag))\n",
    "            else:\n",
    "                lemmatized_tokens.append(word.lower())\n",
    "                \n",
    "        lemmatized_text += \" \".join(lemmatized_tokens)\n",
    "            \n",
    "    return lemmatized_text\n",
    "\n",
    "# removing of all unwanted characters in the text\n",
    "def remove_special_characters(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "# removing all functional words\n",
    "def remove_functional_words(text, stopword_list):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "646cb04a-0fed-4435-8219-68d376ad16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A central function to preprocess to acheieve normalization using above helper functions\n",
    "# The text content is lemmatized, then special characters are removed\n",
    "# Finally stop words (functional) words are removed\n",
    "\n",
    "def normalize_text(source_df: pd.DataFrame, normalized_df: pd.DataFrame, start=0, stop=1000):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tagged_data = treebank.tagged_sents()\n",
    "    nb_tagger = ClassifierBasedPOSTagger(train=tagged_data, classifier_builder=NaiveBayesClassifier.train)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')    \n",
    "    n_data = []\n",
    "    \n",
    "    for inx, row in source_df.iterrows():\n",
    "        if inx < start:\n",
    "            continue;\n",
    "            \n",
    "        if inx > stop:\n",
    "            break;\n",
    "        \n",
    "        text = lemmatize_text(row['content'], nb_tagger, wnl)\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_functional_words(text, stopwords)\n",
    "        \n",
    "        data_row = [row['id'], row['source'], text]\n",
    "        normalized_df.loc[len(normalized_df)] = data_row\n",
    "\n",
    "# invoke normalize_text and save in a CSV file for future processing        \n",
    "norm_df = pd.DataFrame(columns=['id', 'source', 'content'])\n",
    "normalize_text(news_df, norm_df, 0, 20000)\n",
    "#norm_df.to_csv((data_dir + \"/normalized.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28e5950b-fce6-4512-8a48-6bcabebf8a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>gemunu say bus fare revision break news daily ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>six standouts set light critical stage cwc23 q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>eight foreigner arrest heroin break news daily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>member appoint ec hrc break news daily mirror ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>chief inspector police kill homagama accident ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>300 medical graduate complete internship say a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>banking system burden epf fund touch cb govern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>nmra lack professor pharmacology month break n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>presumed human remain find within wreckage tit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>udeni rajapaksa appoint new air force chief br...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id          source  \\\n",
       "0  1.670000e+18  Dailymirror_SL   \n",
       "1  1.670000e+18  Dailymirror_SL   \n",
       "2  1.670000e+18  Dailymirror_SL   \n",
       "3  1.670000e+18  Dailymirror_SL   \n",
       "4  1.670000e+18  Dailymirror_SL   \n",
       "5  1.670000e+18  Dailymirror_SL   \n",
       "6  1.670000e+18  Dailymirror_SL   \n",
       "7  1.670000e+18  Dailymirror_SL   \n",
       "8  1.670000e+18  Dailymirror_SL   \n",
       "9  1.670000e+18  Dailymirror_SL   \n",
       "\n",
       "                                             content  \n",
       "0  gemunu say bus fare revision break news daily ...  \n",
       "1  six standouts set light critical stage cwc23 q...  \n",
       "2  eight foreigner arrest heroin break news daily...  \n",
       "3  member appoint ec hrc break news daily mirror ...  \n",
       "4  chief inspector police kill homagama accident ...  \n",
       "5  300 medical graduate complete internship say a...  \n",
       "6  banking system burden epf fund touch cb govern...  \n",
       "7  nmra lack professor pharmacology month break n...  \n",
       "8  presumed human remain find within wreckage tit...  \n",
       "9  udeni rajapaksa appoint new air force chief br...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first 10 normalized text\n",
    "\n",
    "normalized_df = pd.read_csv((data_dir + \"/normalized1.csv\"), encoding='latin1', engine='python')\n",
    "normalized_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b283f10e-f97f-425e-98d7-c0c9b0acd0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Dailymirror_SL': 6223,\n",
       "         'colombotelegrap': 774,\n",
       "         'NewsfirstSL': 3602,\n",
       "         'theisland_lk': 21,\n",
       "         'NewsWireLK': 4534,\n",
       "         'FT_SriLanka': 1794,\n",
       "         'DailyNews_lk': 16,\n",
       "         'CeylonToday': 1396})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "normalized_df['source'].value_counts().plot(kind='bar')\n",
    "counter = Counter(normalized_df['source'])\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3620c0ba-b316-4384-a591-39a94fbc3c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question (b) handling data imbalance\n",
    "# Among the 18360 datasets obtained, the dataset is shuffled\n",
    "# In the next cell 5000 random samples are obtained using the function written here \n",
    "# sampler = SMOTE | RandomOverSampler | RandomUnderSampler\n",
    "# vectorizer = TfidfVectorizer | CountVectorizer\n",
    "\n",
    "normalized_df = shuffle(normalized_df, random_state=42)\n",
    "\n",
    "# this function will resample with the preferred vectorizer and sampler\n",
    "def handle_sample_imbalance(X, y, vectorizer, sampler):\n",
    "    features = vectorizer.fit_transform(X)\n",
    "    #features = X_tf.toarray()\n",
    "    return sampler.fit_resample(features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "288197cc-379f-4ff0-9b11-6dd54a6363ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalanced sample distribution - Counter({'Dailymirror_SL': 1392, 'NewsWireLK': 966, 'NewsfirstSL': 768, 'FT_SriLanka': 364, 'CeylonToday': 333, 'colombotelegrap': 175, 'theisland_lk': 2})\n",
      "\n",
      "Balanced sample distribution - Counter({'FT_SriLanka': 1392, 'Dailymirror_SL': 1392, 'NewsfirstSL': 1392, 'NewsWireLK': 1392, 'colombotelegrap': 1392, 'CeylonToday': 1392, 'theisland_lk': 1392})\n"
     ]
    }
   ],
   "source": [
    "# random_state can be kept 42 if necessary\n",
    "sample_df = normalized_df.sample(n=5000, replace=True)\n",
    "train_X, test_X, train_y, test_y = train_test_split(sample_df[\"content\"], sample_df[\"source\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# counting on the training labels\n",
    "counter = Counter(train_y)\n",
    "print(\"Imbalanced sample distribution - %s\" % counter)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "sampler = RandomOverSampler(random_state=42)\n",
    "sparse_train_X, train_y = handle_sample_imbalance(train_X, train_y.values, vectorizer, sampler)\n",
    "\n",
    "counter = Counter(train_y)\n",
    "print(\"\\nBalanced sample distribution - %s\" % counter)\n",
    "\n",
    "# removing unused variables for memory issue\n",
    "del counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9eb5e8f-9c7b-4175-a16e-8ab5505f3fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data feature dimension - 10880 rows x 47505 features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>japan</th>\n",
       "      <th>budget</th>\n",
       "      <th>tamil</th>\n",
       "      <th>catholic</th>\n",
       "      <th>church</th>\n",
       "      <th>economy</th>\n",
       "      <th>english</th>\n",
       "      <th>police</th>\n",
       "      <th>women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013778</td>\n",
       "      <td>0.089143</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070960</td>\n",
       "      <td>0.040717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134080</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041290</td>\n",
       "      <td>0.026012</td>\n",
       "      <td>0.050235</td>\n",
       "      <td>0.014214</td>\n",
       "      <td>0.010722</td>\n",
       "      <td>0.012614</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10876</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10877</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10878</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10879</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10880 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       japan  budget     tamil  catholic    church   economy   english  \\\n",
       "0        0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.013778   \n",
       "1        0.0     0.0  0.046457  0.000000  0.000000  0.000000  0.000000   \n",
       "2        0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3        0.0     0.0  0.041290  0.026012  0.050235  0.014214  0.010722   \n",
       "4        0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...      ...     ...       ...       ...       ...       ...       ...   \n",
       "10875    0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "10876    0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "10877    0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "10878    0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "10879    0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         police     women  \n",
       "0      0.089143  0.000000  \n",
       "1      0.070960  0.040717  \n",
       "2      0.134080  0.000000  \n",
       "3      0.012614  0.000000  \n",
       "4      0.000000  0.000000  \n",
       "...         ...       ...  \n",
       "10875  0.000000  0.000000  \n",
       "10876  0.000000  0.000000  \n",
       "10877  0.000000  0.000000  \n",
       "10878  0.000000  0.000000  \n",
       "10879  0.000000  0.000000  \n",
       "\n",
       "[10880 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question (3) sparse and dense vector representation\n",
    "# Using TfidfVectorizer for sparse vector\n",
    "# 5000 samples were taken from above collected dataset and split 90% for training 10% testing\n",
    "# Training samples (5000 * 10/100) upsampled to handle data imbalance\n",
    "# Sparse vectors has more 0s and of high dimension: check the dimension printed underneath\n",
    "\n",
    "# Sparse vector TF-IDF. Few features and corresponding values printed\n",
    "# eg. index no-10547 words like catholic, economy, police and the probabilities shown \n",
    "df = pd.DataFrame(sparse_train_X.todense(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"Sample data feature dimension - %s rows x %s features\" % df.shape)\n",
    "df[['japan', 'budget', 'tamil', 'catholic', 'church', 'economy', 'english', 'police', 'women']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "427f96ea-39df-4341-a37f-20e579dc899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature identified 6685\n",
      "Dimension of features : 300 rows x 6685 features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bishop', 0.924637496471405),\n",
       " ('tiran', 0.6562369465827942),\n",
       " ('easter', 0.6412863731384277),\n",
       " ('1045', 0.6108831763267517),\n",
       " ('alles', 0.6046870946884155),\n",
       " ('attack', 0.5726611614227295),\n",
       " ('terror', 0.569987952709198),\n",
       " ('anthony', 0.5535997152328491),\n",
       " ('bomb', 0.5307350158691406),\n",
       " ('church', 0.5202903747558594)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question (3) sparse and dense vector representation\n",
    "# Using Word2Vec for dense vector model building\n",
    "# Word2Vec is effective in finding relationship between the words\n",
    "# Good technique to reduce dimension, undergiven is the dimension of dense vector\n",
    "# min_count = Any word occurs less than 30 times are ignored, \n",
    "# vector_size = Each feature will have 300 float numbers computed\n",
    "# window = Words are compered on a contextual meaning 100 words before and 100 after\n",
    "\n",
    "# memory clearning for unused objects\n",
    "del df\n",
    "\n",
    "token_train_X = [nltk.word_tokenize(text) for text in train_X.values]\n",
    "\n",
    "w2v_model = Word2Vec(token_train_X, vector_size=300, window=100, min_count=30, sample=1e-3)\n",
    "print(\"Number of feature identified %s\" % len(w2v_model.wv.key_to_index))\n",
    "print(\"Dimension of features : %s rows x %s features\" % (300, len(w2v_model.wv.key_to_index)))\n",
    "\n",
    "# printing similar words\n",
    "similar_words = w2v_model.wv.most_similar('catholic')\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65004ce5-4069-4525-a3d9-4a195ea25bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question (d) Training the classifiers with non-deep learning algorithms\n",
    "# Helper functions to train, predict and measure the accuracy is defined here \n",
    "\n",
    "# Accepts any classifier and builds and model and does the prediction on test data\n",
    "def train_predict_classification(classifier, train_features, train_labels, test_features):\n",
    "    # build model\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    return classifier.predict(test_features)\n",
    "\n",
    "\n",
    "# Model performance is reported\n",
    "def get_prediction_metrics_accuracy(true_labels, predicted_labels):\n",
    "    print('Accuracy:', np.round(metrics.accuracy_score(true_labels, predicted_labels), 4))\n",
    "    print('Precision:', np.round(metrics.precision_score(true_labels, predicted_labels,\n",
    "                                                         average='weighted', zero_division=True), 4))\n",
    "    print('Recall:', np.round(metrics.recall_score(true_labels, predicted_labels,\n",
    "                                                   average='weighted', zero_division=True), 4))\n",
    "    print('F1 Score:', np.round(metrics.f1_score(true_labels, predicted_labels,\n",
    "                                                 average='weighted', zero_division=True), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0777154e-950c-489f-8497-104adf33b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question (d) Training the classifiers with non-deep learning algorithms\n",
    "# Dataset size is 18360 thus broken into random batches(default=5000) and \n",
    "# data imbalance handled, iteratively trained and tested multiple times\n",
    "# normalized_df = shuffled dataset\n",
    "# vectorizer = TfidfVectorizer | CountVectorizer\n",
    "# sampler = SMOTE | RandomOverSampler\n",
    "# classifier = MultinomialNB | SGDClassifier or any classifier\n",
    "# iterations = number of training/testing cycles\n",
    "# n_samples = randomly selected number of samples from 18360 data points\n",
    "\n",
    "def random_sample_prediction(normalized_df, vectorizer, sampler, \n",
    "                                      classifier, iterations=5, n_samples=5000):\n",
    "    true_y = []\n",
    "    predicted_y = []\n",
    "    for n in range(iterations):\n",
    "        # take a random sample on each iteration\n",
    "        sample_df = normalized_df.sample(n=n_samples, replace=True)\n",
    "        \n",
    "        # split the sample randomly at 90% training 10% testing\n",
    "        train_X, test_X, train_y, test_y = train_test_split(sample_df[\"content\"], \n",
    "                                                            sample_df[\"source\"], test_size=0.1) #random_state=42\n",
    "\n",
    "        # handle imbalance on the training set\n",
    "        sparse_train_X, train_y = handle_sample_imbalance(train_X, train_y.values, vectorizer, sampler)\n",
    "        \n",
    "        # transform the test data\n",
    "        test_features = vectorizer.transform(test_X)\n",
    "        \n",
    "        # predict with a classifier\n",
    "        y_hat = train_predict_classification(classifier, sparse_train_X, train_y, test_features)\n",
    "        \n",
    "        # collect and return the true labels and predicted labels to compute accuracy\n",
    "        true_y = true_y + list(test_y)\n",
    "        predicted_y = predicted_y + list(y_hat)\n",
    "    \n",
    "    return true_y, predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01d1ca8e-cb4e-4fdb-908f-521fb2fe2916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9945\n",
      "Precision: 0.9947\n",
      "Recall: 0.9945\n",
      "F1 Score: 0.9945\n"
     ]
    }
   ],
   "source": [
    "# Question (d) Training the classifiers with non-deep learning algorithms\n",
    "# Sparse Representation\n",
    "# SGDClassifier - Performs very well with classification problems\n",
    "\n",
    "classifier = SGDClassifier(loss='hinge', max_iter=100)\n",
    "true_y, predicted_y = random_sample_prediction(normalized_df, vectorizer, \n",
    "                                                        sampler, classifier, 4)\n",
    "get_prediction_metrics_accuracy(true_y, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e69be58a-3901-46c3-aeea-d754a9decdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.997\n",
      "Precision: 0.997\n",
      "Recall: 0.997\n",
      "F1 Score: 0.997\n"
     ]
    }
   ],
   "source": [
    "# Question (d) Training the classifiers with non-deep learning algorithms\n",
    "# Sparse Representation\n",
    "# Random Forest Classifier - Performs well with high dimention vectors, fast and high accuracy\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "true_y, predicted_y = random_sample_prediction(normalized_df, vectorizer, \n",
    "                                                        sampler, classifier, 4)\n",
    "get_prediction_metrics_accuracy(true_y, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7e66507-edfe-4fe4-b850-26754a964317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9735\n",
      "Precision: 0.979\n",
      "Recall: 0.9735\n",
      "F1 Score: 0.9749\n"
     ]
    }
   ],
   "source": [
    "# Question (d) Training the classifiers with non-deep learning algorithms\n",
    "# Sparse Representation\n",
    "# MultinomialNB - Fast performing, high accuracy after RandomForest and SGD\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "true_y, predicted_y = random_sample_prediction(normalized_df, vectorizer, \n",
    "                                                        sampler, classifier, 4)\n",
    "get_prediction_metrics_accuracy(true_y, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c1b69c7-aff8-4391-bd86-ef5b8b54a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question (d) Training the classifiers with non-deep learning algorithms\n",
    "# Dense Representation\n",
    "# Functions to classify with Word2Vec \n",
    "\n",
    "# helper function\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "# helper function   \n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.index_to_key)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "# main function that calls above function and does the classification\n",
    "# with the provided classifier\n",
    "def w2v_random_sample_prediction(normalized_df, classifier, iterations=5, n_samples=5000):\n",
    "    \n",
    "    true_y = []\n",
    "    predicted_y = []\n",
    "    for n in range(iterations):\n",
    "        # take a random sample on each iteration\n",
    "        sample_df = normalized_df.sample(n=n_samples, replace=True)\n",
    "        \n",
    "        # split the sample randomly at 90% training 10% testing\n",
    "        train_X, test_X, train_y, test_y = train_test_split(sample_df[\"content\"], \n",
    "                                                            sample_df[\"source\"], test_size=0.1) #random_state=42\n",
    "        # tokenizing training and testing corpus\n",
    "        token_train_X = [nltk.word_tokenize(text) for text in train_X.values]\n",
    "        token_test_X = [nltk.word_tokenize(text) for text in test_X.values]\n",
    "        \n",
    "        # model building\n",
    "        w2v_model = Word2Vec(token_train_X, vector_size=300, window=100, min_count=30, sample=1e-3)\n",
    "        \n",
    "        train_features = averaged_word_vectorizer(corpus=token_train_X, model=w2v_model.wv,\n",
    "                                                 num_features=300)                   \n",
    "        test_features = averaged_word_vectorizer(corpus=token_test_X, model=w2v_model.wv,\n",
    "                                                num_features=300)\n",
    "        y_hat = train_predict_classification(classifier, train_features, train_y.values, test_features)\n",
    "        \n",
    "        # collect and return the true labels and predicted labels to compute accuracy\n",
    "        true_y = true_y + list(test_y)\n",
    "        predicted_y = predicted_y + list(y_hat)\n",
    "    \n",
    "    return true_y, predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67d1100e-4ff9-46ea-9ba1-2f6eb62642c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.994\n",
      "Precision: 0.9942\n",
      "Recall: 0.994\n",
      "F1 Score: 0.9933\n"
     ]
    }
   ],
   "source": [
    "# Question (d) Training the classifiers with non-deep learning algorithms\n",
    "# Dense Representation with SGDClassifier\n",
    "\n",
    "classifier = SGDClassifier(loss='hinge', max_iter=100)\n",
    "true_y, predicted_y = w2v_random_sample_prediction(normalized_df, classifier, 4, 5000)\n",
    "\n",
    "get_prediction_metrics_accuracy(true_y, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34d43f17-aa31-426a-ba0b-3a42e555ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.996\n",
      "Precision: 0.9961\n",
      "Recall: 0.996\n",
      "F1 Score: 0.9958\n"
     ]
    }
   ],
   "source": [
    "# Question (d) Training the classifiers with non-deep learning algorithms\n",
    "# Dense Representation with Random Forest Classifier\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "true_y, predicted_y = w2v_random_sample_prediction(normalized_df, classifier, 4, 5000)\n",
    "\n",
    "get_prediction_metrics_accuracy(true_y, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "002f2b1d-0234-4b3e-9d71-f721d759d3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9885\n",
      "Precision: 0.9895\n",
      "Recall: 0.9885\n",
      "F1 Score: 0.9887\n"
     ]
    }
   ],
   "source": [
    "# Question (d) Training the classifiers with non-deep learning algorithms\n",
    "# Dense Representation with LinearDiscriminantAnalysis Classifier\n",
    "\n",
    "classifier = LinearDiscriminantAnalysis()\n",
    "true_y, predicted_y = w2v_random_sample_prediction(normalized_df, classifier, 4, 5000)\n",
    "\n",
    "get_prediction_metrics_accuracy(true_y, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4de60203-a3d4-48d0-b6a0-846736484af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
    "\n",
    "normalized_df = shuffle(normalized_df, random_state=42)\n",
    "\n",
    "sample_df = normalized_df.sample(n=5000, replace=True)\n",
    "train_X, test_X, train_y, test_y = train_test_split(sample_df[\"content\"], sample_df[\"source\"], test_size=0.1, random_state=42)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_X)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af692c45-173e-4d21-a1af-738b42c9ba05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 94s 827ms/step - loss: 0.2785 - accuracy: 0.8850 - val_loss: 0.0097 - val_accuracy: 0.9967\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 92s 814ms/step - loss: 0.0032 - accuracy: 0.9983 - val_loss: 0.0074 - val_accuracy: 0.9967\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 89s 788ms/step - loss: 8.1586e-04 - accuracy: 0.9994 - val_loss: 0.0074 - val_accuracy: 0.9956\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 89s 786ms/step - loss: 4.3925e-04 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 0.9944\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 89s 791ms/step - loss: 2.5919e-04 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 0.9944\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 90s 795ms/step - loss: 1.7610e-04 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 0.9956\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 89s 785ms/step - loss: 1.2185e-04 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 0.9956\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 89s 786ms/step - loss: 8.9094e-05 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 0.9956\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 93s 821ms/step - loss: 6.9630e-05 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 0.9956\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 90s 793ms/step - loss: 5.6778e-05 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 0.9956\n"
     ]
    }
   ],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_X)\n",
    "max_length = max(len(seq) for seq in train_sequences)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_X)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# label adjustment to accomodate all 10 types of sources with one-hot encoding\n",
    "handlers = [\"Dailymirror_SL\", \"colombotelegrap\", \"NewsfirstSL\", \"theisland_lk\", \"NewsWireLK\", \n",
    "            \"DailyNews_lk\", \"FT_SriLanka\", \"CeylonToday\"]\n",
    "train_label = list(train_y) + handlers\n",
    "train_label = pd.get_dummies(train_label).values\n",
    "train_label = train_label[0: (len(train_label) - len(handlers))]\n",
    "\n",
    "test_label = list(test_y) + handlers\n",
    "test_label = pd.get_dummies(test_label).values\n",
    "test_label = test_label[0: (len(test_label) - len(handlers))]\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 400, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_padded, train_label, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0269bd4f-9e57-427f-bfec-b590a5ea3efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 82ms/step - loss: 0.0087 - accuracy: 0.9960\n",
      "Testset Loss: 0.009  Accuracy: 0.996\n"
     ]
    }
   ],
   "source": [
    "test_matrix = model.evaluate(test_padded, test_label)\n",
    "print('Testset Loss: {:0.3f}  Accuracy: {:0.3f}'.format(test_matrix[0], test_matrix[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372366b7-605a-4cce-b6d5-ba0f1c9e1b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
