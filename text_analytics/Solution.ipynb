{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7aa0251-d8b5-4f74-8625-cf7d360fc828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all libraries upfront\n",
    "\n",
    "#general utilities\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "from datetime import datetime, date\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#twitter data fetching\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "#NLTK specific libraries\n",
    "import nltk\n",
    "from nltk.corpus import treebank, wordnet as wn\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86f91b73-a31b-4dfb-9aed-ff25f9b3c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing necessary data\n",
    "\n",
    "#initialize handler dataframe\n",
    "handlers = {\"handlers\":[\"Dailymirror_SL\", \"colombotelegrap\", \"NewsfirstSL\", \"theisland_lk\", \"NewsWireLK\", \n",
    "            \"adaderana\", \"DailyNews_lk\", \"TimesOnlineLK\", \"FT_SriLanka\", \"CeylonToday\"]}\n",
    "\n",
    "handlers_df = pd.DataFrame(handlers)\n",
    "handlers_df[\"followers\"] = 0\n",
    "handlers_df[\"friends\"] = 0\n",
    "handlers_df[\"tweets\"] = 0\n",
    "\n",
    "# tweets retrieval periods\n",
    "to_date = datetime(2023, 6, 29)\n",
    "from_date = datetime(to_date.year - 1, to_date.month, to_date.day)\n",
    "\n",
    "# data save and retrieval directory\n",
    "data_dir = r'C:\\Users\\SujithThillymplam\\Documents\\machine-learn-experiment\\text_analytics'\n",
    "\n",
    "# additional REST endpoint headers\n",
    "api_headers = {\"Authorization\": \"Bearer AAAAAAAAAAAAAAAAAAAAANRILgAAAAAAnNwIzUejRCOuH5E6I8xnZz4puTs%3D1Zv7ttfk8LF81IUq16cHjhLTvJu4FA33AGWWjCpTnA\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b871a8eb-3458-47d2-9351-3d9564059a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to generate summary counts for followers, friends and number of tweets for each handler \n",
    "\n",
    "# function to count followers and friends\n",
    "def followers_friends_count(df, header_map={}, option='friends'):\n",
    "    for inx, row in df.iterrows():\n",
    "        url = \"https://api.twitter.com/1.1/\" + option + \"/ids.json?screen_name=\" + row[\"handlers\"] + \"&count=5000\"\n",
    "        fjson = requests.get(url, headers=api_headers).json()\n",
    "        fcount = len(fjson[\"ids\"])\n",
    "        next_cursor = fjson[\"next_cursor\"] \n",
    "    \n",
    "        while next_cursor > 0:\n",
    "            sub_url = url + \"&cursor=\" + str(fjson[\"next_cursor\"])\n",
    "            fjson = requests.get(sub_url, headers=header_map).json()\n",
    "            next_cursor = fjson[\"next_cursor\"]\n",
    "            fcount += len(fjson[\"ids\"])\n",
    "        \n",
    "        df.at[inx, option] = fcount\n",
    "        print(\"%s : %s - %s\" % (option, row[\"handlers\"], fcount))\n",
    "        \n",
    "# function to count tweets for a given period of time for all news handlers        \n",
    "def tweet_counts_for_periods(df, from_date, to_date):\n",
    "    \n",
    "    for inx, row in df.iterrows():\n",
    "        tweet_count = 0;\n",
    "        query = '@' + row[\"handlers\"]\n",
    "        tquery = \"%s lang:en until:%s since:%s\" % (query, to_date.strftime(\"%Y-%m-%d\"), from_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        itr = sntwitter.TwitterSearchScraper(tquery).get_items()\n",
    "        for tweet in itr:\n",
    "            tweet_count += 1\n",
    "        \n",
    "        df.at[inx, \"tweets\"] = tweet_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58c6d1d-a1e4-4f01-b5ee-b78113337e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the above function and generate the summary\n",
    "# note - all 3 function execution is time consuming, executed individually and saved in a CSV file for later reference\n",
    "\n",
    "followers_friends_count(handlers_df, api_headers)\n",
    "followers_friends_count(handlers_df, api_headers, 'followers')\n",
    "tweet_counts_for_periods(handlers_df, from_date, to_date)\n",
    "\n",
    "# handlers_df.to_csv (data_dir + \"\\handlers.csv\", index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74c662bf-3d10-41a4-b699-20c0a9d53386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handlers</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>588872</td>\n",
       "      <td>35</td>\n",
       "      <td>34677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>colombotelegrap</td>\n",
       "      <td>42406</td>\n",
       "      <td>2</td>\n",
       "      <td>1198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsfirstSL</td>\n",
       "      <td>525162</td>\n",
       "      <td>8</td>\n",
       "      <td>20932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theisland_lk</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "      <td>4743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsWireLK</td>\n",
       "      <td>222255</td>\n",
       "      <td>2</td>\n",
       "      <td>39698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adaderana</td>\n",
       "      <td>533456</td>\n",
       "      <td>14</td>\n",
       "      <td>9114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DailyNews_lk</td>\n",
       "      <td>14858</td>\n",
       "      <td>93</td>\n",
       "      <td>4314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TimesOnlineLK</td>\n",
       "      <td>17092</td>\n",
       "      <td>7</td>\n",
       "      <td>2736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FT_SriLanka</td>\n",
       "      <td>80772</td>\n",
       "      <td>8</td>\n",
       "      <td>2857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CeylonToday</td>\n",
       "      <td>62797</td>\n",
       "      <td>33</td>\n",
       "      <td>3972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          handlers  followers  friends  tweets\n",
       "0   Dailymirror_SL     588872       35   34677\n",
       "1  colombotelegrap      42406        2    1198\n",
       "2      NewsfirstSL     525162        8   20932\n",
       "3     theisland_lk        261        0    4743\n",
       "4       NewsWireLK     222255        2   39698\n",
       "5        adaderana     533456       14    9114\n",
       "6     DailyNews_lk      14858       93    4314\n",
       "7    TimesOnlineLK      17092        7    2736\n",
       "8      FT_SriLanka      80772        8    2857\n",
       "9      CeylonToday      62797       33    3972"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question (a) - 10 popular news sources\n",
    "# news handlers|no of followers|no of followings|no of tweets for last 1 year\n",
    "# Read from handlers.csv file and display the stats\n",
    "# The values obtained here are generated by executing the above given functions (time consuming)\n",
    "\n",
    "handlers_df = pd.read_csv(data_dir + \"\\handlers.csv\")\n",
    "handlers_df[[\"handlers\", \"followers\", \"friends\", \"tweets\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d92dd8d-080b-4ffe-a0c7-a5b00850045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the newslinks for every every handler\n",
    "\n",
    "def find_tweets_with_news_links(handler_list, from_date, to_date, size=0):\n",
    "    filtered_tweets = []\n",
    "    \n",
    "    for handler_name in handler_list:\n",
    "        tquery = \"%s lang:en until:%s since:%s\" % (('@' + handler_name), to_date.strftime(\"%Y-%m-%d\"), from_date.strftime(\"%Y-%m-%d\"))\n",
    "        tweets = sntwitter.TwitterSearchScraper(tquery).get_items()\n",
    "        web_url_regex = \"(https?://t\\.co/([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)\"\n",
    "\n",
    "        for t in tweets:\n",
    "            urls = re.findall(web_url_regex, t.rawContent)\n",
    "            news_url = ''\n",
    "            for x in urls:\n",
    "                news_url = x[0].strip()\n",
    "                filtered_tweets.append([t.id, handler_name, t.date, news_url])\n",
    "            \n",
    "            if(size > 0 and len(filtered_tweets) > size):\n",
    "                break\n",
    "    \n",
    "        if(size > 0 and len(filtered_tweets) > size):\n",
    "            break\n",
    "            \n",
    "    headers = ['id', 'source', 'date', 'url']\n",
    "    return pd.DataFrame(filtered_tweets, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6a6d0e-5cf2-44e9-97a1-8ebfbfe407ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the news extractor and generate newslinks\n",
    "# save to a CSV file for future reference so that the contents can be retrieved later\n",
    "\n",
    "news_df = find_tweets_with_news_links(handlers[\"handlers\"], from_date, to_date)\n",
    "#news_df.to_csv (data_dir + \"\\\\news_contents.csv\", index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd4ef725-60da-490e-b47c-cdb807ed40a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 11:19:41+00:00</td>\n",
       "      <td>https://t.co/f9WwIKUC6k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 11:19:01+00:00</td>\n",
       "      <td>https://t.co/osPbuVCcd5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 11:07:33+00:00</td>\n",
       "      <td>https://t.co/aANaVO4QcW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 08:04:39+00:00</td>\n",
       "      <td>https://t.co/8c8Wu4kGPy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 08:07:02+00:00</td>\n",
       "      <td>https://t.co/IPFcbc0pVH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:44:42+00:00</td>\n",
       "      <td>https://t.co/9WFsFwkQWU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:44:01+00:00</td>\n",
       "      <td>https://t.co/vEsOHG1z5w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:43:13+00:00</td>\n",
       "      <td>https://t.co/2ilBkJNFa7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:42:29+00:00</td>\n",
       "      <td>https://t.co/GDXR3AFJdW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:41:48+00:00</td>\n",
       "      <td>https://t.co/05eKZ6PwK3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id          source                       date  \\\n",
       "0  1.67E+18  Dailymirror_SL  2023-06-29 11:19:41+00:00   \n",
       "1  1.67E+18  Dailymirror_SL  2023-06-29 11:19:01+00:00   \n",
       "2  1.67E+18  Dailymirror_SL  2023-06-29 11:07:33+00:00   \n",
       "3  1.67E+18  Dailymirror_SL  2023-06-29 08:04:39+00:00   \n",
       "4  1.67E+18  Dailymirror_SL  2023-06-29 08:07:02+00:00   \n",
       "5  1.67E+18  Dailymirror_SL  2023-06-29 05:44:42+00:00   \n",
       "6  1.67E+18  Dailymirror_SL  2023-06-29 05:44:01+00:00   \n",
       "7  1.67E+18  Dailymirror_SL  2023-06-29 05:43:13+00:00   \n",
       "8  1.67E+18  Dailymirror_SL  2023-06-29 05:42:29+00:00   \n",
       "9  1.67E+18  Dailymirror_SL  2023-06-29 05:41:48+00:00   \n",
       "\n",
       "                       url  \n",
       "0  https://t.co/f9WwIKUC6k  \n",
       "1  https://t.co/osPbuVCcd5  \n",
       "2  https://t.co/aANaVO4QcW  \n",
       "3  https://t.co/8c8Wu4kGPy  \n",
       "4  https://t.co/IPFcbc0pVH  \n",
       "5  https://t.co/9WFsFwkQWU  \n",
       "6  https://t.co/vEsOHG1z5w  \n",
       "7  https://t.co/2ilBkJNFa7  \n",
       "8  https://t.co/GDXR3AFJdW  \n",
       "9  https://t.co/05eKZ6PwK3  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display first 10 saved news urls to verify \n",
    "\n",
    "news_df =  pd.read_csv((data_dir + \"/news_contents.csv\"), engine='python', encoding='latin1')\n",
    "news_df[[\"id\", \"source\", \"date\", \"url\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3915a82-b700-4db3-bc3e-8a1aa6b0f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanding the contraction texts, underneath is a helper function to expand them \n",
    "contraction_map = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# expanding the shorthand words\n",
    "def expand_contractions(sentence, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "    return expanded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df21d571-828d-4816-83b5-2146606ed61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question (b) - extract all the articles from the links discovered above\n",
    "\n",
    "# helper function which retrievs texts for a given url\n",
    "# it extracts texts for the givel htmltags and specific attributes in the tags\n",
    "\n",
    "def extract_web_text(url, tags=['p'], filterAttr={}):\n",
    "    text = ''\n",
    "    html = None\n",
    "    soup = None\n",
    "    title = None\n",
    "    body = None\n",
    "    \n",
    "    try:\n",
    "        html = requests.get(url, timeout=60).content\n",
    "        soup = bs(html, 'html.parser')\n",
    "        title = soup.find('head')\n",
    "        body = soup.find('body')\n",
    "    \n",
    "    except:\n",
    "        print(\"Connectivity error with news url: %s\" % url)\n",
    "        return ''\n",
    "    \n",
    "    if title != None:\n",
    "        for c in title.find_all(tags, filterAttr):\n",
    "            text += c.text.strip()\n",
    "    \n",
    "    if body != None:\n",
    "        for c in body.find_all(tags, filterAttr):\n",
    "            content = re.sub(r'[\\t\\r\\n]', r' ', c.text.strip())\n",
    "            \n",
    "            if len(content) < 19 or (len(content) > 18 and re.search(r\"\\s\", content)):\n",
    "                text += ' ' + content\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "# helper function to remove all unwanted characters from the content\n",
    "# it also expands the contractions by calling expand_contractions() function defined above\n",
    "\n",
    "def clean_text_content(content):\n",
    "    \n",
    "    content = re.sub(r\"[\\'|’]\", \"'\", content.strip())\n",
    "    content = re.sub(r\"[“|”]\", \"'\", content.strip())\n",
    "    content = re.sub(r\"[?|$|&|*|%|@|(|)|~|©|\\\\|™]\", r' ', content)\n",
    "    content = re.sub(r'[\\s]+', r' ', content)\n",
    "    content = expand_contractions(content, contraction_map)\n",
    "    \n",
    "    return content\n",
    "\n",
    "# main function that is called to extract all the links\n",
    "def extract_clean_text(handler_df, links_df, start=0, stop=1000):\n",
    "    handler_map = {}\n",
    "    \n",
    "    for inx, row in handler_df.iterrows():\n",
    "        handler_attr = {}\n",
    "        handler_attr[\"name\"] = row[\"name\"]\n",
    "        handler_attr[\"tags\"] = row[\"content_tags\"].split()\n",
    "        handler_map[row[\"handlers\"]] = handler_attr\n",
    "    \n",
    "    for inx, row in links_df.iterrows():\n",
    "        if inx < start:\n",
    "            continue;\n",
    "            \n",
    "        if inx > stop:\n",
    "            break;\n",
    "        \n",
    "        handler_attr = handler_map[row[\"source\"]]\n",
    "        content = extract_web_text(row[\"url\"], handler_attr[\"tags\"])\n",
    "        content = clean_text_content(content)\n",
    "        print(\"%s url - %s extracted\" % (inx, row[\"url\"]))\n",
    "        \n",
    "        if handler_attr[\"name\"] in content:\n",
    "            links_df.at[inx, \"content\"] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f252790-d8a3-46e2-b67c-7f4ac338757e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 url - https://t.co/f9WwIKUC6k extracted\n",
      "1 url - https://t.co/osPbuVCcd5 extracted\n",
      "2 url - https://t.co/aANaVO4QcW extracted\n",
      "3 url - https://t.co/8c8Wu4kGPy extracted\n",
      "4 url - https://t.co/IPFcbc0pVH extracted\n",
      "5 url - https://t.co/9WFsFwkQWU extracted\n",
      "6 url - https://t.co/vEsOHG1z5w extracted\n",
      "7 url - https://t.co/2ilBkJNFa7 extracted\n",
      "8 url - https://t.co/GDXR3AFJdW extracted\n",
      "9 url - https://t.co/05eKZ6PwK3 extracted\n",
      "10 url - https://t.co/5bkWcAVkW3 extracted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 11:19:41+00:00</td>\n",
       "      <td>https://t.co/f9WwIKUC6k</td>\n",
       "      <td>Gemunu says no to bus fare revision - Breaking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 11:19:01+00:00</td>\n",
       "      <td>https://t.co/osPbuVCcd5</td>\n",
       "      <td>Six standouts set to light up the critical sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 11:07:33+00:00</td>\n",
       "      <td>https://t.co/aANaVO4QcW</td>\n",
       "      <td>Eight foreigners arrested with heroin - Breaki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 08:04:39+00:00</td>\n",
       "      <td>https://t.co/8c8Wu4kGPy</td>\n",
       "      <td>Members appointed to EC and HRC - Breaking New...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 08:07:02+00:00</td>\n",
       "      <td>https://t.co/IPFcbc0pVH</td>\n",
       "      <td>Chief Inspector of police killed in Homagama a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:44:42+00:00</td>\n",
       "      <td>https://t.co/9WFsFwkQWU</td>\n",
       "      <td>300 medical graduates who completed internship...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:44:01+00:00</td>\n",
       "      <td>https://t.co/vEsOHG1z5w</td>\n",
       "      <td>Banking system will not be burdened, EPF funds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:43:13+00:00</td>\n",
       "      <td>https://t.co/2ilBkJNFa7</td>\n",
       "      <td>NMRA lacked professor in pharmacology for mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:42:29+00:00</td>\n",
       "      <td>https://t.co/GDXR3AFJdW</td>\n",
       "      <td>'Presumed human remains' found within wreckage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.670000e+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>2023-06-29 05:41:48+00:00</td>\n",
       "      <td>https://t.co/05eKZ6PwK3</td>\n",
       "      <td>Udeni Rajapaksa to be appointed as new Air For...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id          source                       date  \\\n",
       "0  1.670000e+18  Dailymirror_SL  2023-06-29 11:19:41+00:00   \n",
       "1  1.670000e+18  Dailymirror_SL  2023-06-29 11:19:01+00:00   \n",
       "2  1.670000e+18  Dailymirror_SL  2023-06-29 11:07:33+00:00   \n",
       "3  1.670000e+18  Dailymirror_SL  2023-06-29 08:04:39+00:00   \n",
       "4  1.670000e+18  Dailymirror_SL  2023-06-29 08:07:02+00:00   \n",
       "5  1.670000e+18  Dailymirror_SL  2023-06-29 05:44:42+00:00   \n",
       "6  1.670000e+18  Dailymirror_SL  2023-06-29 05:44:01+00:00   \n",
       "7  1.670000e+18  Dailymirror_SL  2023-06-29 05:43:13+00:00   \n",
       "8  1.670000e+18  Dailymirror_SL  2023-06-29 05:42:29+00:00   \n",
       "9  1.670000e+18  Dailymirror_SL  2023-06-29 05:41:48+00:00   \n",
       "\n",
       "                       url                                            content  \n",
       "0  https://t.co/f9WwIKUC6k  Gemunu says no to bus fare revision - Breaking...  \n",
       "1  https://t.co/osPbuVCcd5  Six standouts set to light up the critical sta...  \n",
       "2  https://t.co/aANaVO4QcW  Eight foreigners arrested with heroin - Breaki...  \n",
       "3  https://t.co/8c8Wu4kGPy  Members appointed to EC and HRC - Breaking New...  \n",
       "4  https://t.co/IPFcbc0pVH  Chief Inspector of police killed in Homagama a...  \n",
       "5  https://t.co/9WFsFwkQWU  300 medical graduates who completed internship...  \n",
       "6  https://t.co/vEsOHG1z5w  Banking system will not be burdened, EPF funds...  \n",
       "7  https://t.co/2ilBkJNFa7  NMRA lacked professor in pharmacology for mont...  \n",
       "8  https://t.co/GDXR3AFJdW  'Presumed human remains' found within wreckage...  \n",
       "9  https://t.co/05eKZ6PwK3  Udeni Rajapaksa to be appointed as new Air For...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question (b) - extract all the articles by calling above functions\n",
    "# following code extracts content from first 10 links and prints them underneath\n",
    "\n",
    "extract_clean_text(handlers_df, news_df, start=0, stop=10)\n",
    "news_df[[\"id\", \"source\", \"date\", \"url\", \"content\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "750a7846-d0f0-4149-b005-ec946cff327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question (b) - function to produce token summary from the above extracted text\n",
    "# This produces no_tokens, no_unique_tokens, no_news_items per newshandler as output\n",
    "\n",
    "def handler_token_summary(handler_df, news_df, start=0, stop=1000):\n",
    "    handler_map = {}\n",
    "    \n",
    "    for inx, row in handler_df.iterrows():\n",
    "        handler_attr = {}\n",
    "        handler_attr[\"no_tokens\"] = 0\n",
    "        handler_attr[\"no_unique_tokens\"] = 0\n",
    "        handler_attr[\"no_news_items\"] = 0\n",
    "        handler_attr[\"unique_tokens\"] = set()\n",
    "        handler_map[row[\"handlers\"]] = handler_attr\n",
    "    \n",
    "    for inx, row in news_df.iterrows():\n",
    "        tokens = nltk.word_tokenize(str(row[\"content\"]))\n",
    "        if row[\"source\"] in handler_map:\n",
    "            handler_attr = handler_map[row[\"source\"]]\n",
    "            handler_attr[\"unique_tokens\"].update(tokens)\n",
    "            handler_attr[\"no_tokens\"] += len(tokens)\n",
    "            handler_attr[\"no_unique_tokens\"] = len(handler_attr[\"unique_tokens\"])\n",
    "            handler_attr[\"no_news_items\"] += 1\n",
    "            handler_map[row[\"source\"]] = handler_attr\n",
    "        \n",
    "        if inx < start:\n",
    "            continue;\n",
    "            \n",
    "        if inx > stop:\n",
    "            handler_attr.pop(\"unique_tokens\")\n",
    "            handler_map[row[\"source\"]] = handler_attr\n",
    "            break;\n",
    "    \n",
    "    return handler_map\n",
    "\n",
    "stats = handler_token_summary(handlers_df, news_df, 0, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db916ea1-a858-4aa4-86e7-f5da32ef8c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handler</th>\n",
       "      <th>no_news_items</th>\n",
       "      <th>no_tokens</th>\n",
       "      <th>no_unique_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>6222</td>\n",
       "      <td>7822119</td>\n",
       "      <td>98964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>colombotelegrap</td>\n",
       "      <td>777</td>\n",
       "      <td>2780678</td>\n",
       "      <td>87700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsfirstSL</td>\n",
       "      <td>3524</td>\n",
       "      <td>11898190</td>\n",
       "      <td>36665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theisland_lk</td>\n",
       "      <td>21</td>\n",
       "      <td>48416</td>\n",
       "      <td>5401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsWireLK</td>\n",
       "      <td>4532</td>\n",
       "      <td>2337881</td>\n",
       "      <td>42052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adaderana</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DailyNews_lk</td>\n",
       "      <td>16</td>\n",
       "      <td>778</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TimesOnlineLK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FT_SriLanka</td>\n",
       "      <td>1794</td>\n",
       "      <td>1557234</td>\n",
       "      <td>42086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CeylonToday</td>\n",
       "      <td>430</td>\n",
       "      <td>387484</td>\n",
       "      <td>14171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           handler  no_news_items  no_tokens  no_unique_tokens\n",
       "0   Dailymirror_SL           6222    7822119             98964\n",
       "1  colombotelegrap            777    2780678             87700\n",
       "2      NewsfirstSL           3524   11898190             36665\n",
       "3     theisland_lk             21      48416              5401\n",
       "4       NewsWireLK           4532    2337881             42052\n",
       "5        adaderana              0          0                 0\n",
       "6     DailyNews_lk             16        778                71\n",
       "7    TimesOnlineLK              0          0                 0\n",
       "8      FT_SriLanka           1794    1557234             42086\n",
       "9      CeylonToday            430     387484             14171"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question (b) Display token summary by handlers\n",
    "# The above handler_token_summary function output is formatted for display\n",
    "\n",
    "summary = []\n",
    "for k, v in stats.items():\n",
    "    summary.append([k, v[\"no_news_items\"], v[\"no_tokens\"], v[\"no_unique_tokens\"]])\n",
    "    \n",
    "summary_df = pd.DataFrame(summary, columns=[\"handler\", \"no_news_items\" , \"no_tokens\", \"no_unique_tokens\"])\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cc0f981-bddf-438f-baee-8ebe65216e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question (b) Data preparation for model building\n",
    "# The under defined helper functions would be combined with the extracted dataset\n",
    "\n",
    "# Lemmatizing the text\n",
    "def lemmatize_text(content, tagger, lemmatizer):\n",
    "    sentences = nltk.sent_tokenize(content)\n",
    "    lemmatized_text = ''\n",
    "    \n",
    "    for s in sentences:\n",
    "        tokens = [t.strip() for t in nltk.word_tokenize(s)]\n",
    "        \n",
    "        tagged_token = tagger.tag(tokens)\n",
    "        lemmatized_tokens = []\n",
    "        \n",
    "        for word, tag in tagged_token:\n",
    "            custom_tag = None\n",
    "            \n",
    "            if tag.startswith('J'):\n",
    "                custom_tag = wn.ADJ\n",
    "            elif tag.startswith('V'):\n",
    "                custom_tag = wn.VERB\n",
    "            elif tag.startswith('N'):\n",
    "                custom_tag = wn.NOUN\n",
    "            elif tag.startswith('R'):\n",
    "                custom_tag = wn.ADV\n",
    "            #else:\n",
    "                #custom_tag = None\n",
    "            \n",
    "            if custom_tag:\n",
    "                lemmatized_tokens.append(lemmatizer.lemmatize(word.lower(), custom_tag))\n",
    "            else:\n",
    "                lemmatized_tokens.append(word.lower())\n",
    "                \n",
    "        lemmatized_text += \" \".join(lemmatized_tokens)\n",
    "            \n",
    "    return lemmatized_text\n",
    "\n",
    "# removing of all unwanted characters in the text\n",
    "def remove_special_characters(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "# removing all functional words\n",
    "def remove_functional_words(text, stopword_list):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "646cb04a-0fed-4435-8219-68d376ad16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def normalize_text(source_df: pd.DataFrame, normalized_df: pd.DataFrame, start=0, stop=1000):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tagged_data = treebank.tagged_sents()\n",
    "    nb_tagger = ClassifierBasedPOSTagger(train=tagged_data, classifier_builder=NaiveBayesClassifier.train)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')    \n",
    "    n_data = []\n",
    "    \n",
    "    for inx, row in source_df.iterrows():\n",
    "        if inx < start:\n",
    "            continue;\n",
    "            \n",
    "        if inx > stop:\n",
    "            break;\n",
    "        \n",
    "        text = lemmatize_text(row['content'], nb_tagger, wnl)\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_functional_words(text, stopwords)\n",
    "        \n",
    "        data_row = [row['id'], row['source'], text]\n",
    "        normalized_df.loc[len(normalized_df)] = data_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28e5950b-fce6-4512-8a48-6bcabebf8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = pd.DataFrame(columns=['id', 'source', 'content'])\n",
    "x = normalize_text(news_df, norm_df, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3620c0ba-b316-4384-a591-39a94fbc3c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>gemunu say bus fare revision break news daily ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>six standouts set light critical stage cwc23 q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>eight foreigner arrest heroin break news daily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>member appoint ec hrc break news daily mirror ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>chief inspector police kill homagama accident ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.67E+18</td>\n",
       "      <td>Dailymirror_SL</td>\n",
       "      <td>300 medical graduate complete internship say a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id          source                                            content\n",
       "0  1.67E+18  Dailymirror_SL  gemunu say bus fare revision break news daily ...\n",
       "1  1.67E+18  Dailymirror_SL  six standouts set light critical stage cwc23 q...\n",
       "2  1.67E+18  Dailymirror_SL  eight foreigner arrest heroin break news daily...\n",
       "3  1.67E+18  Dailymirror_SL  member appoint ec hrc break news daily mirror ...\n",
       "4  1.67E+18  Dailymirror_SL  chief inspector police kill homagama accident ...\n",
       "5  1.67E+18  Dailymirror_SL  300 medical graduate complete internship say a..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288197cc-379f-4ff0-9b11-6dd54a6363ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
